{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сегментация легких ###\n",
    "\n",
    "### Анамнез ###\n",
    "1. В целом, легкие представляют из себя два связных темных пятна на снимках\n",
    "2. Тем не менее, на ряде изображений этот критерий существенно нарушается: иногда сердце имеет практически такую же проницеамость и отличимо лишь по контурам; иногда в легкие вдаются костные выросты (грудина? искривленный позвоночник), которые на метках исключены. В то же время, например, ключицы входят в область легких, т.к. находятся \"перед\" ними. Поэтому полагаться только на интенсивность нельзя.\n",
    "3. Изображений достаточно много, чтобы, с учетом аугментаций, натренировать небольшую сеть.\n",
    "\n",
    "**Задачу будем решать двумя независимыми способами: при помощи нейросети и классическим алгоритмом**\n",
    "\n",
    "### Архитектура сети ###\n",
    "\n",
    "Нейросетевая часть очень похожа на задание 1. Все так же UNet4, по сути, единственный вариант, доступный по вычислительной мощности и способности обучаться на малом наборе данных. \n",
    "\n",
    "Для тренировки из изображения вырезается случайный кусок от 0.8 до 1.0 площади исходного изображения, и масштабируется до 128*128. Это несколько насыщает тренировочные данные, не изменяя их типичных особенностей. Например, нерезка на малые куски или отражения наверняка лишь ухудшили бы результат, так как распределение валидационных данных обладало бы общими особенностями (e.g. сердце слева), которыми не обладал бы тренировочный датасет.\n",
    "\n",
    "Я тренировал сеть при помощи `Adam`, `lr=0.01`, коэффициент затухания `lr=0.9999`, 1 эпоха == длине тренировочного датасета (52 шт), батч = 4 изображения. Результаты проверялись после 100 и 300 эпох тренировки. 300 эпох занимает на моей машине ~45 минут.\n",
    "\n",
    "### Метрики ###\n",
    "На мой взгляд, для масок крупных объектов хорошей метрикой для бинарной сегментации является IOU: ее высокие значения обеспечивают, что маски \"геометрически\" близки. При этом, например, ошибка в виде тонкой линии (которую впоследствии легко исправить пост-процессингом, как в первом задании), внесет существенно меньший вклад, чем круглое пятно (которое, например, на границе областей устранить непросто). Это отвечает геометрической интуиции понятия \"хорошего решения\".\n",
    "\n",
    "Также я вычисляю попиксельный F1_score в основном потому, что он остался у меня от первой задачи.и его легко считать. Для бинарной классификации есть и более аккуратные метрики, например коэффициент Мэтьюса. На сколько мне известно, F1 ведет себя *принципиально неадекватно* в случае существенного дисбаланса классов. На картинках, где легкие занимают примерно 1/3 площади и она должна вполне хорошо выражать успешность того или иного алгоритма. Опять же, время не резиновое и часть углов приходится срезать. На практике метрику все равно пришлось бы выбирать прагматически, исходя из имеющейся задачи. Например, если важна мажоранта области, нужно с большим весом учитывать ложно отрицательные пиксели, если миноранта -- то ложно положительные и т.д.\n",
    "Опять, же достаточно хорошими геометрическими инвариантами являются площадь и периметр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data2\n",
    "import modeltools2\n",
    "import os\n",
    "from unet import UNet\n",
    "import torch, torch.utils, torch.utils.data\n",
    "from torchsummary import summary\n",
    "import modeltools1\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import lungsdetector\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"../02_image_segmentation2/xray-lung-segmentation\"\n",
    "train_file = \"idx-trn0.txt\"\n",
    "val_file = \"idx-val0.txt\"\n",
    "\n",
    "# out_dir = \"out2\"\n",
    "nn_outdir = \"out2/nn\"\n",
    "algo_outdir = \"out2/algo\"\n",
    "if not os.path.exists(nn_outdir) or not os.path.exists(algo_outdir): \n",
    "    os.makedirs(nn_outdir)\n",
    "    os.makedirs(algo_outdir)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "weights_path = None\n",
    "\n",
    "# comment all lines below to train the UNet from scratch\n",
    "# weights after 300 epochs of training\n",
    "weights_path = \"lungs_weights300.dat\"\n",
    "\n",
    "# weights after 100 epochs of training\n",
    "# weights_path = \"lungs_weights100.dat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data2.LungsDataset(data_dir, train_file)\n",
    "train_dataset.train()\n",
    "val_dataset = data2.LungsDataset(data_dir, val_file)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 4, num_workers=2, \n",
    "    pin_memory=True)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 4, num_workers=2, \n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             320\n",
      "              ReLU-2         [-1, 32, 128, 128]               0\n",
      "       BatchNorm2d-3         [-1, 32, 128, 128]              64\n",
      "            Conv2d-4         [-1, 32, 128, 128]           9,248\n",
      "              ReLU-5         [-1, 32, 128, 128]               0\n",
      "       BatchNorm2d-6         [-1, 32, 128, 128]              64\n",
      "         MaxPool2d-7           [-1, 32, 64, 64]               0\n",
      "         DownBlock-8  [[-1, 32, 64, 64], [-1, 32, 128, 128]]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          18,496\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-11           [-1, 64, 64, 64]             128\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
      "        MaxPool2d-15           [-1, 64, 32, 32]               0\n",
      "        DownBlock-16  [[-1, 64, 32, 32], [-1, 64, 64, 64]]               0\n",
      "           Conv2d-17          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-19          [-1, 128, 32, 32]             256\n",
      "           Conv2d-20          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-22          [-1, 128, 32, 32]             256\n",
      "        MaxPool2d-23          [-1, 128, 16, 16]               0\n",
      "        DownBlock-24  [[-1, 128, 16, 16], [-1, 128, 32, 32]]               0\n",
      "           Conv2d-25          [-1, 256, 16, 16]         295,168\n",
      "             ReLU-26          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-27          [-1, 256, 16, 16]             512\n",
      "           Conv2d-28          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-29          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-30          [-1, 256, 16, 16]             512\n",
      "        DownBlock-31  [[-1, 256, 16, 16], [-1, 256, 16, 16]]               0\n",
      "  ConvTranspose2d-32          [-1, 128, 32, 32]         131,200\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-34          [-1, 128, 32, 32]             256\n",
      "      Concatenate-35          [-1, 256, 32, 32]               0\n",
      "           Conv2d-36          [-1, 128, 32, 32]         295,040\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "           Conv2d-39          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-41          [-1, 128, 32, 32]             256\n",
      "          UpBlock-42          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-43           [-1, 64, 64, 64]          32,832\n",
      "             ReLU-44           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-45           [-1, 64, 64, 64]             128\n",
      "      Concatenate-46          [-1, 128, 64, 64]               0\n",
      "           Conv2d-47           [-1, 64, 64, 64]          73,792\n",
      "             ReLU-48           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-49           [-1, 64, 64, 64]             128\n",
      "           Conv2d-50           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-51           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-52           [-1, 64, 64, 64]             128\n",
      "          UpBlock-53           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-54         [-1, 32, 128, 128]           8,224\n",
      "             ReLU-55         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-56         [-1, 32, 128, 128]              64\n",
      "      Concatenate-57         [-1, 64, 128, 128]               0\n",
      "           Conv2d-58         [-1, 32, 128, 128]          18,464\n",
      "             ReLU-59         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-60         [-1, 32, 128, 128]              64\n",
      "           Conv2d-61         [-1, 32, 128, 128]           9,248\n",
      "             ReLU-62         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-63         [-1, 32, 128, 128]              64\n",
      "          UpBlock-64         [-1, 32, 128, 128]               0\n",
      "           Conv2d-65          [-1, 2, 128, 128]              66\n",
      "================================================================\n",
      "Total params: 1,928,322\n",
      "Trainable params: 1,928,322\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 131.00\n",
      "Params size (MB): 7.36\n",
      "Estimated Total Size (MB): 138.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels = 1, out_channels=2, n_blocks = 4).to(device)\n",
    "summary(model, (1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optim_params = model.parameters()\n",
    "optim = torch.optim.Adam(optim_params, lr=lr)\n",
    "loss = modeltools1.FocalLoss(reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma =0.9999)\n",
    "epochs = 10 #optimal value is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "if (\"weights_path\" in locals() or \"weights_path\" in globals()) and weights_path:\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "        train_dataset.train()\n",
    "        model.train()\n",
    "        modeltools1.train_model(model, loss, optim, scheduler, train_dataloader,\n",
    "                num_epochs = epochs, device = device) \n",
    "        model.cpu()\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        torch.save(model.state_dict(), './lungs_weights'+current_time+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже сетка прогоняется на тренировочном и валидационных датасетах. В папке out2 сохранаются исходное изображение (для наглядности), предсказанная маска и исходная метка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d22ffb629d4989a52d1dbf10717223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train F1 score 0.9749588588988078, IOU score: 0.9511719683431229\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb186fdac2c416c985fad0a2027d45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val F1 score 0.9585478880011972, IOU score: 0.9266433694832762\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "train_dataset.eval()\n",
    "val_dataset.eval()\n",
    "\n",
    "#note, we output 128*128 images for they are escriptive enough\n",
    "F1_score, IOU_score = modeltools2.eval_model_on_dataloader(model, train_dataloader, device, f\"{nn_outdir}/img_train\")\n",
    "print(f\"train F1 score {F1_score}, IOU score: {IOU_score}\")   \n",
    "F1_score, IOU_score = modeltools2.eval_model_on_dataloader(model, val_dataloader, device, f\"{nn_outdir}/img_val\")\n",
    "print(f\"val F1 score {F1_score}, IOU score: {IOU_score}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты ###    \n",
    "    \n",
    "    Эпох    | F1 train  | IOU train | F1 val    | IOU val  | Loss\n",
    "    10      | 0.844359  | 0.7507    | 0.820202  | 0.723840 | 0.0147\n",
    "    100     | 0.968350  | 0.938900  | 0.956945  | 0.923821 | 0.0075\n",
    "    300     | 0.9749    | 0.9511    | 0.9585    | 0.9266   | 0.0037"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глазометрическое изучение результатов показало, что разница между предсказаниями сетки после 100 и 300 эпох тренировки мало существенна. Из плюсов, это говорит в пользу адекватности наших метрик, они тоже изменились несильно. Из минусов, надежда, что ту же сеть можно натренировать на тех же данных лучше, слабая. Ниже привожу несколько примеров работы сетки (300 эпох) на валидационном датасете.\n",
    "\n",
    "Зеленым отмечены примеры незначительных ошибок, которые элементарно устраняются пост-процессингом. Красным -- ошибки, которые устранить сложнее. В основном они возникают на снимках с уникальными анатомическими особенностями: сетке просто не хватило таких данных в трейн-датасете. Ну и одно изображение просто супер неконтрастное, а я не делаю никакой входной коррекции, равно как и соответствующих аугментаций. Результат кажется мне достаточно хорошим.\n",
    "![](lungs_demo1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эвристический алгоритм ###\n",
    "\n",
    "На моей машине инференс проходит со скоростью 8 снимков в секунду вместе с подсчетом метрик и сохранением файлов и в целом сеть дает хороший результат, но эту задачу можно решать и прямым алгоритмом. Это дает больший контроль над морфологией объекта, поэтому в некоторых случах может быть предпочтительным. \n",
    "\n",
    "Поскольку в классических алгоритмах обработки изображений я новичок и не претендую на то, чтобы восполнить этот пробел за два дня (хотя в процессе я проникся и таки его восполню), я предъявляю к нему всего два требования:\n",
    "1. он должен быть параметризован для всего набора данных, то есть я не против подбора некоторых параметров, но они должны подбираться один раз и работать приемлемо на большинстве входных изображений\n",
    "2. результат должен быть визуально приемлемым. На выходе должно быть ровно две области и IOU с меткой > 0.8\n",
    "\n",
    "Я попробовал скормить эти картинки ванильному MSER (в реализации opencv 4.5.5), но выявилось несколько проблем: а) он сильно зависит от контрастности изображения. Что гораздо хуже, в силу своей природы, MSER объединяет области в объекты несколькими способами. Так, например, появляются объекты класса легкое+сердце, легкое, легкое без куска и придумать простой алгоритм различения \"правильного варианта\" мне не удалось. Более того, не видя исходного снимка, я бы и сам не взялся сделать такой выбор. Вот пример объектов, которые MSER находит на одном и том же изоражении. Кроме первого, это все вариации левого легкого.\n",
    "![](lungs_demo2.png)\n",
    "\n",
    "Поэтому я решил все же использовать более примитивный алгоритм на основе метода Отсу как начального приближения. После суточного марафона по перестановке кусочков получилось следующее.\n",
    "\n",
    "1. находим черный цвет на изображении и заменяем его средним по оставшемуся изображению. Это частично решает проблему черной рамки, которая сбивает метод Отсу с толку\n",
    "2. подкручиваем контраст, обрезая гистограмму (я взял готовую реализацию, источник указан в коде)\n",
    "3. запускаем метод Отсу, находим начальный threshold по яркости\n",
    "4. разбираем изображение на компоненты связности, выбираем две с наибольшей площадью\n",
    "5. если число компонет меньше двух (например, легкие склеились в центре) или если одна из компонент связна с границей изображения, уменьшаем threshold, повторяем последние два шага\n",
    "6. используем замыкание и раздутие, чтобы получить более гладкую маску.\n",
    "\n",
    "Следующий фрагмент выбирает случайное изображение из тестового набора и сохраняет для него все шаги алгоритма в папку `algo_outdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(train_dataset) - 1)\n",
    "train_dataset.numpy()\n",
    "img, lbl = train_dataset[i]\n",
    "img = lungsdetector.channels_last(img)\n",
    "lbl = lungsdetector.channels_last(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8858357105431681 0.7950674\n"
     ]
    }
   ],
   "source": [
    "rslt = lungsdetector.separate_lungs(img, 10, 40, 25, 3, steps_file = f'{algo_outdir}/rnd')\n",
    "if rslt is not None:\n",
    "    rslt_filt = lungsdetector.smoothen(rslt, 10, 2) \n",
    "    cv2.imwrite(f'{algo_outdir}/rnd_filtered_result.png', rslt_filt)\n",
    "    cv2.imwrite(f'{algo_outdir}/rnd_filtered_label.png', ((lbl > 0) *255).astype(np.uint8))\n",
    "    f1 = lungsdetector.F1_single_input(rslt_filt, lbl)\n",
    "    iou = lungsdetector.IOU_single_metrics(rslt_filt, lbl)\n",
    "    print(f1,iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример работы ###\n",
    "![](lungs_demo3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76806a796f246e6a32ff25a17d16f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't separate image: 60\n",
      "Can't separate image: 65\n",
      "Can't separate image: 67\n",
      "Can't separate image: 87\n",
      "Can't separate image: 124\n",
      "Can't separate image: 153\n",
      "Can't separate image: 168\n",
      "train F1 score 0.8501824767535043, IOU score: 0.7524567438334954, failed to classify: 7, failure rate: 0.0330188679245283\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50355385c88445da3b9305a2b89b974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't separate image: 20\n",
      "Can't separate image: 35\n",
      "Can't separate image: 48\n",
      "Can't separate image: 51\n",
      "Can't separate image: 52\n",
      "Can't separate image: 56\n",
      "Can't separate image: 68\n",
      "Can't separate image: 125\n",
      "Can't separate image: 127\n",
      "val F1 score 0.8605183551878992, IOU score: 0.7639141593660627, failed to classify: 9, failure rate: 0.06338028169014084\n"
     ]
    }
   ],
   "source": [
    "train_dataset.numpy()\n",
    "val_dataset.numpy()\n",
    "\n",
    "#F1 and IOU are computed only for successfully segmented images\n",
    "f1, iou, failed = lungsdetector.eval_on_dataset(train_dataset, otsu_step = 2, \n",
    "    black_limit = 40, clip_percent = 25, border_thickness = 3, closing_rad = 10, dilation_rad = 2,\n",
    "    max_steps = 60, out_file = f'{algo_outdir}/test', verbose = False)\n",
    "print(f\"train F1 score {f1}, IOU score: {iou}, failed to classify: {failed}, failure rate: {failed / len(train_dataset)}\") \n",
    "\n",
    "f1, iou, failed = lungsdetector.eval_on_dataset(val_dataset, otsu_step = 2, \n",
    "    black_limit = 40, clip_percent = 25, border_thickness = 3, closing_rad = 10, dilation_rad = 2,\n",
    "    max_steps = 60, out_file = f'{algo_outdir}/test', verbose = False)\n",
    "print(f\"val F1 score {f1}, IOU score: {iou}, failed to classify: {failed}, failure rate: {failed / len(val_dataset)}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы ###\n",
    "В целом, не могу сказать, что алгоритм получился  удачным, ни с точки зрения метрик, ни при визуальной оценке. Заявленное условие IOU>0.8 (которе UNet достигает чуть и не за 10 эпох обучения) в среднем, увы, не достигнуто. Частые проблемы: объединение областей в результате финальной фильтрации, либо наоборот чрезмерная редукция в попытке отделиться от границы изображения. При этом увеличение шага threshold (`otsu_step`) приводит к улучшению по первому пункту и ухудшению по второму, а уменьшение радиуса раздутия (`dilation_rad`), наоборот. В целом сетевая реализация работает гораздо лучше.\n",
    "\n",
    "Подбор параметров в качестве ablation study показал, что все части алгоритма существенны, но детальный анализ выходит за рамки моих возможностей по времени.\n",
    "\n",
    "**Пример работы:**\n",
    "\n",
    "![](lungs_demo4.png)\n",
    "\n",
    "### Что не получилось ###\n",
    "Изначально я намеревался применять метод Отсу один раз, а склеившиеся объекты (как правило, легкое и мусор у границы) разделять watershedding'ом, но не смог правильно вычислить начальные маркеры: ни точки минимума градиента (что должно отвечать \"однородно залитым областям\"), ни точки максимума расстояний от границы не подошли. Таких точек слишком много и watershedding выдает лоскутное одеяло. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bc1f8c35bd18689d84db61d33d5c58e75d3454474558482af5176b6a1531d50"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
