{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5493f7",
   "metadata": {},
   "source": [
    "## Топографическая сегментация ##\n",
    "\n",
    "\n",
    "### Маленькое предисловие ###\n",
    "\n",
    "*Сейчас вечер вторника и случился хм... факап. Как вы знаете, в выданных данных были проблемы: сначала на валидации не было меток, потом они появились, но не совпадали с картинками по размеру. Короче, я только что добавил костыль, чтобы картиники ресайзились до маски и выяснил, что ровно те картинки в val set'е, которые были кривые, отличаются по цвету.... Почему я этого не заметил раньше ГЛАЗАМИ, я не понимаю, увидел только по метрике, когда починил датасет. Это 100% мой косяк, и увы, сильно снижает качество представленного решения.*\n",
    "\n",
    "*Само по себе, наличие таких картинок -- небольшая беда. Нужно просто подобрать аугментации и/или препроцессинг. Я попробовал пару вариантов, больше не успеваю: проверка каждого конфига занимает полтора часа.*\n",
    "\n",
    "*Исходное решение, не учитывающее существование картинок с разными цветовыми статистиками дает средние значения F1 на train и val датасете 0.91 и 0.57.*\n",
    "\n",
    "*Первый вариант аугментаций: добавить jitter и нормализовать rgb-каналы. Получили 0.89 и 0.68. Это уже лучше, т.к. видно, что аугментации работают, но абсолютное значение 0.68 -- крайне упадочное, картинка тоже выходит плохая. Поэтому такой вариант нам не вариант.*\n",
    "\n",
    "*Второй вариант: без jitter, но нормализация происходит в пространстве hsv (покрутив ручки в gimp мне показалось, что это более адекватно представляет имеющиеся отличия). Результат: 0.92 к 0.63, тоже нехорошо.*\n",
    "\n",
    "*Третий вариант: jitter, затем нормализация hsv дал 0.88 и 0.65*. \n",
    "\n",
    "*На первый взгляд, нельзя назвать эти эксперименты успешным. Но изучение примеров обработанных изображений позволяет предположить, что аугментации правильные, но не хватает глубины сети: ей просто не хватает вариативности, чтобы отвязатся от стандартного распределения цветов. Подробнее об этом -- в последнем блоке. Еще идеи: запустить что-то типа auto_gamma на входе для всех изображений; нормализовать по всей картинке, вместо фрагментов; запоминать матожидание и дисперсию в разных цветовых пространствах на трейн-сете, потом считать аналоги на одной картине на инференсе и сдвигать к распределению трейна.*\n",
    "\n",
    "*Все тексты и примеры ниже написаны изначально вокруг ситуации, когда все картинки одинаковые с точки зрения статистик цвета. Так как времени аккуратно переписать тексты уже нет, я оставлю в репозитории и старые веса (без нормализации), и новые (с нормализацией). Прямой текст относится к старым весам, про новые буду писать курсивные комментарии.*\n",
    "\n",
    "\n",
    "### Архитектура ###\n",
    "\n",
    "Так как мы имеем дело с сегментацией изображений средних размеров на достаточно неплохо визуально отделимые и относительно гомогенные классы, есть надежда, что с этим справится UNet небольшого размера. Известно, что UNet достаточно хорошо тренируется на малых датасетах. \n",
    "\n",
    "Будем тренировать UNet из 4-х пар блоков. С 5 парами количество параметров выходит за пределы вычислительных возможностей моего компьютера. \n",
    "Мой конфиг: \n",
    "- i5-7300 4@2.5Ghz\n",
    "- 8GB RAM\n",
    "- GTX1050 2Gb\n",
    "- HDD\n",
    "\n",
    "Исходные изображения нарежем на произвольные (почти) квадратные куски и отмасштабируем до размера 128*128. Выбор размера обучающей картинки обусловлен следующим. Для корректной работы используемой реализации UNet требуются входные данные со сторонами, кратными $2^\\text{количество пар блоков}$ (иначе pooling-слои приводят к тому, что выходное разрешение меньше входного). Далее экспериментально подобран размер, обеспечивающий хоть какой-то batching при тренировке.\n",
    "\n",
    "В качестве дополнительных аугментаций применяется случайное отражение по обоим осям. Вращения не применяются, так как реально они не увеличивают разнообразие данных, в то же время лишь добавляют артефактов. Шумы и цветовые смещения также не применяются ввиду относительной однородности тренировочных и валидационных данных. *N.B.: теперь мы знаем, что это не так.*\n",
    "\n",
    "Данные загружаются с диска один раз и в дальнейшем все манипуляции происходят в памяти, чтобы снизить нагрузку на HDD у условиях небольшого набора данных для обучения. Аугментации применяются на лету по расписанию Dataloader'а, нарезанные изображения не хранятся.\n",
    "\n",
    "### Инференс на исходных изображениях ###\n",
    "С целью обеспечения работы модели на исходных изображений реализован тайлер на без пакета `pytorch_toolbelt`. При этом шаг замощения в 2 раза меньше размера плитки, предсказания\n",
    "на перекрывающихся областях усредняются, паддинг осуществляется с отражением.\n",
    "\n",
    "### Метрика и энергия ###\n",
    "Так как для конкретных вырезанных кусочков может наблюдаться существенных перекос по присутствующим\n",
    "классам, отличных от среднего распределения, вместо логарифмического подобия (`CrossEntropy / softmax+NLLLoss`) используется `FocalLoss`, который умеет адаптироваться к дисбалансу классов (на самом деле преобразование корня бывает полезным для выравнивания гистограмм в широчайшем круге задач).\n",
    "\n",
    "Так как в задаче нет существенных прагматических ограничений, что могло бы потребовать приоритезации каких-то из стандартных отношений predicted/ground_truth, будем использовать попиксельный `F1_score`.\n",
    "Так как он считается по каждому из каналов это не создает дисбаланса, как при бинарной классификации. Технически реализована возможность передавать веса для усреднения по классам, но реально я всегда выславляю их 1:1:1\n",
    "\n",
    "### Обучение ###\n",
    "Модель показала приемлемые результаты после 1 часа тренировки (120 000 изображений), поэтому  не стал далее экспериментировать. Интересно, что уже после тренировки на 4000 изображений, на что уходит пара минут, модель выдает уже вполне разумные результаты.\n",
    "\n",
    "Используется оптимизатор ADAM с `lr=0.01` и параметрами по умолчанию в паре с экспоненциальным планировщиком скорости обучения. Коэффициент затухания `0.9999` на эпоху, эпоха состоит из 1 батча в 4 изображения (по одному фрагменту из каждого из исходных изображений).\n",
    "\n",
    "Вычисленные веса приложены, весь код тренировки сохранен.\n",
    "\n",
    "### Постпроцессинг ###\n",
    "Изучение результатов работы сети показало, что она распознает кассы *слишком* точно, выделяя опушки леса как \"прочее\", а лесополосы и деревья на приусадебных участках как \"лес\". В качесте proof of concept преимуществ пост-фильтрации в этой задаче, я включил в пайплайн простенький фильтр на базе `skimage`: замыкание (композиция раздутия и эрозии) и заполнение пустот по пороговому значению площади. Коэффициенты подобраны эмпирически (фильтры работают на cpu и небыстро, поэтому запускать рандомизированный gridsearch не было времени; да и тенденция по среднему изменению F1 была достаточно явной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a15cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data1\n",
    "import torch, torch.utils, torch.utils.data\n",
    "from unet import UNet\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchsummary import summary\n",
    "import modeltools1\n",
    "import datetime\n",
    "import torchvision\n",
    "import os\n",
    "import filters1\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#image output folder\n",
    "out_dir = \"out1\"\n",
    "if not os.path.exists(out_dir): os.mkdir(out_dir)\n",
    "\n",
    "weights_path = None\n",
    "\n",
    "#weights trained on 4 * 30000 image fragments, set to None to force retraining\n",
    "#these are the old weights trined on and for the images with the same \n",
    "# color properties. Thus, you need to set up the transforms accordingly\n",
    "weights_path = \"weights-FocalLoss-30000epochs.dat\"\n",
    "data1.Dataset.TRAIN_TRANSFORMS = [\"randomresizedcrop\", \"tofloat\", \"randomflip\", \"fixlabels\"]\n",
    "data1.Dataset.VAL_TRANSFORMS = [\"tofloat\", \"fixlabels\", \"resize_src_to_labels\"]\n",
    "\n",
    "# The third set of weights works with the default transforms\n",
    "# reload the ipython kernel to restore defaults\n",
    "# comment the lines redefining transforms above to use these weights\n",
    "# weights_path = \"weights_topography_HSVJITNORM_10000.dat\"\n",
    "\n",
    "#paths to datasets\n",
    "train_data_dir = \"../01_image_segmentation1/01_train\"\n",
    "train_data_filename = \"idx-train.txt\"\n",
    "val_data_dir = \"../01_image_segmentation1/02_test_clean\"\n",
    "val_data_filename = \"idx-test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be9cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6e37fa45544c2ab59d5eb109b116f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading raw data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65613ace4b74675968097348659466c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading raw data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train dataset. Upon creation dataset[ind] produces 128*128 images, arbitrarily\n",
    "# chosen from idx'th input image. Thus dataset[0] gives different results it is\n",
    "# called. Accepts an optional size output_size parameter defaulted to (128,128) \n",
    "dataset_train = data1.Dataset(train_data_dir, train_data_filename)\n",
    "\n",
    "dataset_val = data1.Dataset(val_data_dir, val_data_filename)\n",
    "# this method switches a dataset to a raw mode, i.e. after dataset.eval()\n",
    "# dataset[idx] stably produces the idx'th original image\n",
    "dataset_val.eval()\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = 4, num_workers=1, \n",
    "    pin_memory=True, persistent_workers=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_train, batch_size = 4, num_workers=1, \n",
    "    pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85befb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             896\n",
      "              ReLU-2         [-1, 32, 128, 128]               0\n",
      "       BatchNorm2d-3         [-1, 32, 128, 128]              64\n",
      "            Conv2d-4         [-1, 32, 128, 128]           9,248\n",
      "              ReLU-5         [-1, 32, 128, 128]               0\n",
      "       BatchNorm2d-6         [-1, 32, 128, 128]              64\n",
      "         MaxPool2d-7           [-1, 32, 64, 64]               0\n",
      "         DownBlock-8  [[-1, 32, 64, 64], [-1, 32, 128, 128]]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          18,496\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-11           [-1, 64, 64, 64]             128\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
      "        MaxPool2d-15           [-1, 64, 32, 32]               0\n",
      "        DownBlock-16  [[-1, 64, 32, 32], [-1, 64, 64, 64]]               0\n",
      "           Conv2d-17          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-19          [-1, 128, 32, 32]             256\n",
      "           Conv2d-20          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-22          [-1, 128, 32, 32]             256\n",
      "        MaxPool2d-23          [-1, 128, 16, 16]               0\n",
      "        DownBlock-24  [[-1, 128, 16, 16], [-1, 128, 32, 32]]               0\n",
      "           Conv2d-25          [-1, 256, 16, 16]         295,168\n",
      "             ReLU-26          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-27          [-1, 256, 16, 16]             512\n",
      "           Conv2d-28          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-29          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-30          [-1, 256, 16, 16]             512\n",
      "        DownBlock-31  [[-1, 256, 16, 16], [-1, 256, 16, 16]]               0\n",
      "  ConvTranspose2d-32          [-1, 128, 32, 32]         131,200\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-34          [-1, 128, 32, 32]             256\n",
      "      Concatenate-35          [-1, 256, 32, 32]               0\n",
      "           Conv2d-36          [-1, 128, 32, 32]         295,040\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "           Conv2d-39          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-41          [-1, 128, 32, 32]             256\n",
      "          UpBlock-42          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-43           [-1, 64, 64, 64]          32,832\n",
      "             ReLU-44           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-45           [-1, 64, 64, 64]             128\n",
      "      Concatenate-46          [-1, 128, 64, 64]               0\n",
      "           Conv2d-47           [-1, 64, 64, 64]          73,792\n",
      "             ReLU-48           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-49           [-1, 64, 64, 64]             128\n",
      "           Conv2d-50           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-51           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-52           [-1, 64, 64, 64]             128\n",
      "          UpBlock-53           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-54         [-1, 32, 128, 128]           8,224\n",
      "             ReLU-55         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-56         [-1, 32, 128, 128]              64\n",
      "      Concatenate-57         [-1, 64, 128, 128]               0\n",
      "           Conv2d-58         [-1, 32, 128, 128]          18,464\n",
      "             ReLU-59         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-60         [-1, 32, 128, 128]              64\n",
      "           Conv2d-61         [-1, 32, 128, 128]           9,248\n",
      "             ReLU-62         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-63         [-1, 32, 128, 128]              64\n",
      "          UpBlock-64         [-1, 32, 128, 128]               0\n",
      "           Conv2d-65          [-1, 3, 128, 128]              99\n",
      "================================================================\n",
      "Total params: 1,928,931\n",
      "Trainable params: 1,928,931\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 131.12\n",
      "Params size (MB): 7.36\n",
      "Estimated Total Size (MB): 138.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels = 3, out_channels=3, n_blocks = 4).to(device)\n",
    "summary(model, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa89742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optim_params = model.parameters()\n",
    "optim = torch.optim.Adam(optim_params, lr=lr)\n",
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "loss = modeltools1.FocalLoss(reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma =0.9999)\n",
    "epochs = 10000 #set to at least 30 000 to reproduce the pretrained weights performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c050f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model is loaded if path to weights is specified, otherwise trains itself\n",
    "losses = []\n",
    "if (\"weights_path\" in locals() or \"weights_path\" in globals()) and weights_path:\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    dataset_train.train()\n",
    "    modeltools1.train_model(model, loss, optim, scheduler, dataloader_train,\n",
    "        num_epochs = epochs, device = device) #30 000 takes one hour and suffices\n",
    "    model.cpu()\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    torch.save(model.state_dict(), './weights_topography_'+current_time+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce27c33",
   "metadata": {},
   "source": [
    "Проверим на одном изображении.\n",
    "\n",
    "Я провожу большинство проверок на train'е, так как на момент вечера воскресенья, валидационный датасет все еще был битый (маски не совпадают по размеру с исходным изораженим)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ec68c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c468dbc8be543c5b167c259a0178211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8979603275315912\n"
     ]
    }
   ],
   "source": [
    "dataset_train.eval()\n",
    "image = dataset_train[0][0]\n",
    "label = dataset_train[0][1]\n",
    "dataset_train.eval_no_norm()\n",
    "orig_img = dataset_train[0][0]\n",
    "\n",
    "label_masks = modeltools1.classes_to_masks(label)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "prediction = modeltools1.tiled_eval(model, image, 16, (128,128), (64,64), device, device)\n",
    "pred_mask = modeltools1.logits_to_masks(prediction)\n",
    "masked = modeltools1.apply_mask(orig_img, pred_mask, colors=modeltools1.DEFAULT_OVERLAY_COLORS, alpha=0.3)\n",
    "torchvision.io.write_png(masked, out_dir+\"/test.png\")\n",
    "score = modeltools1.F1_score(pred_mask, label_masks)\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d00ce",
   "metadata": {},
   "source": [
    "Ну и посчитаем срденее по трейнсету. При этом в папке `out1` появятся: исходные изображения, сгенерированные и эталонные маски, оверлей обоих масок на исходное изображение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78df8c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386f79dc263f411e8c5544e431182279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images in dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9676b773b6427fb38bd34c349a1713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.8979603275315912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1411f93b264f48c98981b1a095a3c115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.9183278604105028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89208d3552854f9c99033bd8d6362fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 2: 0.9096528753406101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d8a89103d04c48a55a4c9a0387562d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 3: 0.9245349943980967\n",
      "Average F1 score on train set: 0.9126190144202002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_train.eval()\n",
    "total_score = modeltools1.eval_on_dataset(model, dataset_train, out_dir+\"/train\", 8,\n",
    "    (128,128), (64,64), device, device)\n",
    "\n",
    "print(f\"Average F1 score on train set: {total_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ed6a7",
   "metadata": {},
   "source": [
    "`Average F1 score on train set: 0.9126190144202002` выглядит уже неплохо, но вот непосредственное изучение результата показывает проблему:\n",
    "![](prob_demo1.png)\n",
    "\n",
    "Сеть распознает классы слишком локально. Понятно, что бОльшую сеть, тренируемую на бОльших фрагментах можно от этого отучить, на текущую архитектуру тут надежды не очень много, поэтому отфильтруем эвристически. Сначала попробуем на одном изображении. Тут можно поподбирать параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e76842e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebcc8a8bb924273913deacba6489d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score: 0.8979603275315912, filtered score: 0.9297780981072274\n"
     ]
    }
   ],
   "source": [
    "dataset_train.eval()\n",
    "image, label = dataset_train[0]\n",
    "prediction = modeltools1.tiled_eval(model, image, 16, (128,128), (64,64), device, device)\n",
    "pred_mask = modeltools1.logits_to_masks(prediction)\n",
    "label_mask = modeltools1.classes_to_masks(label)\n",
    "score = modeltools1.F1_score(pred_mask, label_mask)\n",
    "\n",
    "pred_fitered = filters1.remove_small_holes(pred_mask, 10, 200)\n",
    "filtered_score = modeltools1.F1_score(pred_fitered, label_mask)\n",
    "\n",
    "print(f\"Original score: {score}, filtered score: {filtered_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71a187",
   "metadata": {},
   "source": [
    "Прогресс налицо, поэтому прогоним на всех изображениях. Результат можно наблюдать в папке out. Здесь привожу один пример.\n",
    "\n",
    "*Эта каринка сгенерирована на первом наборе весов*\n",
    "\n",
    "![](prob_demo2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6ea1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe134336b7d4941ab50f81cb734d9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images in dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105a5b4fd8754e7b8f5437519b9aa1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.929835574907831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b56076c9744837a5f97a66595d6cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.930605983005312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0799ac431a4d49904b80db7144c7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 2: 0.9362953003445535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b05d62f0844b80a823fd34f9d1e264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 3: 0.9479652156513637\n",
      "Average F1 score after filtering on train set: 0.9361755184772651\n"
     ]
    }
   ],
   "source": [
    "# Compute F1 with filtration on train dataset\n",
    "dataset_train.eval()\n",
    "total_score = modeltools1.eval_on_dataset(model, dataset_train, out_dir+\"/train\", 8,\n",
    "    (128,128), (64,64), device, device, [], True, 10 ,300)\n",
    "\n",
    "print(f\"Average F1 score after filtering on train set: {total_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6f22d",
   "metadata": {},
   "source": [
    "Так же прогоним на валидационных данных, когда их починят...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5884bf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0218c366872b4abda98e6be5a0339bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images in dataset:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09d25ddc3c34b67a8f8e55d9a005a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.9006673085498438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6da076b45e94f5aafe20a39fe4f9a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.9524839651320565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c8830764fb4861b44f2bb52461e963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 2: 0.4885928129208133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5b8c95b5e94becb8c54e03767e9fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 3: 0.39763728484524546\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00a344d88af4ffdadd8198e9872522b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 4: 0.43877826831096517\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ba5640703c4af9840f454ef23dcc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 5: 0.3676692633556871\n",
      "Average F1 score on val set: 0.5909714838524353\n"
     ]
    }
   ],
   "source": [
    "dataset_val.eval()\n",
    "total_score = modeltools1.eval_on_dataset(model, dataset_val, out_dir+\"/val\", 8,\n",
    "    (128,128), (64,64), device, device, [], True, 10, 300)\n",
    "\n",
    "print(f\"Average F1 score on val set: {total_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe85f",
   "metadata": {},
   "source": [
    "### Выводы ###\n",
    "\n",
    "Видно, что модель дает хорошее начальное приближение для последующей обработки эвристическими методами *ровно до того момента, пока соблюдается цветовое распределение тестовых данных (первые двекартинки из валидации такие, последующие -- нет)*. Предложенная постфильтрация далека от идеала (на самом деле я попробовал первую пришедшую в голову идею и результат меня устроил). \n",
    "\n",
    "В последней момент мне пришла в голову идея вероятностного фильтра для того, чтобы сделать разметку более \"полигональной\", но реализовать я не успеваю. Вместо closing-фильтра сделать следующее: кидать в карту случайным образом маленькие треугольники. Скажем, со сторонами до 10 (20? 30?) пикселей. Если все три вершины одного цвета (на исходной карте без учета уже накиданных треугольников), то на новой карте весь треугольник красится в тот же цвет. Это должно и \"замкнуть\" малые \"дыры\" и сделать границы хотя бы ломанной. В случае наложения двух таких треугольников \"побеждает\" более приоритетный слой (дома над лесами, леса над дефолтом). А потом може еще линеаризовать границы так: запустить детектор границ (Лаплас?), а потом в найденные точки граници покидать отрезки. Дальше смотрим на k-пиксельную окретсность отрезка (k~3-10) и считаем в ней точки разных классов. Если соотношение примерно 1:1:0.01, значит эта прямая разделяет первые два класса два класса. Заливаем полоину ее окретсности в один цвет, вторую -- в другой и вокруг начала и конца чуть сглаживаем углы (гауссом?). \n",
    "\n",
    "*Теперь о ситуации со сменой распределения цветовой информации. В целом, нельзя сказать, что аугментации совсем не помогли. На картинке ниже сверху -- пример изображения из тренировочного датасета, снизу -- пример изображения со смещенными цветовыми характеристиками из валидационного.*\n",
    "![](prob_demo3.png)\n",
    "\n",
    "*Видно, что хоть аугментированные веса и не дают такого же хорошего результата на трейне, на out-of-distribution данных работают гораздо лучше. При этом разницы между аугментированными весами после 10 000, 20 000 и 30 000 эпох почти нет, а функция энергии, хоть и сохраняет тенденцию к убыванию, убывает оооооочень медленно. Это заставляет задуматься, что аугментации правильные, но сеть насытилась. Очень вероятно, что проблема бы решилась добавлением 5-ой пары блоков в UNet, но на это у меня не хватает мощи.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f339b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
