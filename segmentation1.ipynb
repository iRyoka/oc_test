{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5493f7",
   "metadata": {},
   "source": [
    "## Топографическая сегментация ##\n",
    "\n",
    "### Архитектура ###\n",
    "\n",
    "Так как мы имеем дело с сегментацией изображений средних размеров на достаточно неплохо визуально отделимые и относительно гомогенные классы, есть надежда, что с этим справится UNet небольшого размера. Известно, что UNet достаточно хорошо тренируется на малых датасетах. \n",
    "\n",
    "Будем тренировать UNet из 4-х пар блоков. С 5 парами количество параметров выходит за пределы вычислительных возможностей моего компьютера. \n",
    "Мой конфиг: \n",
    "- i5-7300 4@2.5Ghz\n",
    "- 8GB RAM\n",
    "- GTX1050 2Gb\n",
    "- HDD\n",
    "\n",
    "Исходные изображения нарежем на произвольные (почти) квадратные куски и отмасштабируем до размера 128*128. Выбор размера обучающей картинки обусловлен следующим. Для корректной работы используемой реализации UNet требуются входные данные со сторонами, кратными $2^\\text{количество пар блоков}$ (иначе pooling-слои приводят к тому, что выходное разрешение меньше входного). Далее экспериментально подобран размер, обеспечивающий хоть какой-то batching при тренировке.\n",
    "\n",
    "В качестве дополнительных аугментаций применяется случайное отражение по обоим осям. Вращения не применяются, так как реально они не увеличивают разнообразие данных, в то же время лишь добавляют артефактов. Шумы и цветовые смещения также не применяются ввиду относительной однородности тренировочных и валидационных данных.\n",
    "\n",
    "Данные загружаются с диска один раз и в дальнейшем все манипуляции происходят в памяти, чтобы снизить нагрузку на HDD у условиях небольшого набора данных для обучения. Аугментации применяются на лету по расписанию Dataloader'а, нарезанные изображения не хранятся.\n",
    "\n",
    "### Инференс на исходных изображениях ###\n",
    "С целью обеспечения работы модели на исходных изображений реализован тайлер на без пакета `pytorch_toolbelt`. При этом шаг замощения в 2 раза меньше размера плитки, предсказания\n",
    "на перекрывающихся областях усредняются, паддинг осуществляется с отражением.\n",
    "\n",
    "### Метрика и энергия ###\n",
    "Так как для конкретных вырезанных кусочков может наблюдаться существенных перекос по присутствующим\n",
    "классам, отличных от среднего распределения, вместо логарифмического подобия (`CrossEntropy / softmax+NLLLoss`) используется `FocalLoss`, который умеет адаптироваться к дисбалансу классов (на самом деле преобразование корня бывает полезным для выравнивания гистограмм в широчайшем круге задач).\n",
    "\n",
    "Так как в задаче нет существенных прагматических ограничений, что могло бы потребовать приоритезации каких-то из стандартных отношений predicted/ground_truth, будем использовать попиксельный `F1_score`. Технически реализована возможность передавать веса для усреднения по классам, но реально я всегда выславляю их 1:1:1\n",
    "\n",
    "### Обучение ###\n",
    "Модель показала приемлемые результаты после 1 часа тренировки (120 000 изображений), поэтому  не стал далее экспериментировать. Интересно, что уже после тренировки на 4000 изображений, на что уходит пара минут, модель выдает уже вполне разумные результаты.\n",
    "\n",
    "Используется оптимизатор ADAM с `lr=0.01` и параметрами по умолчанию в паре с экспоненциальным планировщиком скорости обучения. Коэффициент затухания `0.9999` на эпоху, эпоха состоит из 1 батча в 4 изображения (по одному фрагменту из каждого из исходных изображений).\n",
    "\n",
    "Вычисленные веса приложены, весь код тренировки сохранен.\n",
    "\n",
    "### Постпроцессинг ###\n",
    "Изучение результатов работы сети показало, что она распознает кассы *слишком* точно, выделяя опушки леса как \"прочее\", а лесополосы и деревья на приусадебных участках как \"лес\". В условиях ограниченного времени я ограничился только двумя фильтрами на базе `skimage`: замыкания (композиция раздутия и эрозии) и заполнения пустот по пороговому значению площади. Коэффициенты подобраны эмпирически (фильтры работают на cpu и небыстро, поэтому запускать рандомизированный gridsearch не было времени; да и тенденция по среднему изменению F1 была достаточно явной)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a15cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data1\n",
    "import torch, torch.utils, torch.utils.data\n",
    "from unet import UNet\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from torchsummary import summary\n",
    "import modeltools1\n",
    "import datetime\n",
    "import torchvision\n",
    "import os\n",
    "import filters1\n",
    "from itertools import product\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#image output folder\n",
    "out_dir = \"out1\"\n",
    "if not os.path.exists(out_dir): os.mkdir(out_dir)\n",
    "\n",
    "#weights trained on 4 * 30000 image fragments, set to None to force retraining\n",
    "weights_path = \"weights-FocalLoss-30000epochs.dat\"\n",
    "#weights_path = None\n",
    "\n",
    "#paths to datasets\n",
    "train_data_dir = \"../01_image_segmentation1/01_train\"\n",
    "train_data_filename = \"idx-train.txt\"\n",
    "val_data_dir = \"../01_image_segmentation1/02_test_clean\"\n",
    "val_data_filename = \"idx-test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be9cd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0f3582519d4943a45e0552f1a5c427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading raw data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0685d42dd5644986aae25c995a60fc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading raw data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train dataset. Upon creation dataset[ind] produces 128*128 images, arbitrarily\n",
    "# chosen from idx'th input image. Thus dataset[0] gives different results it is\n",
    "# called. Accepts an optional size output_size parameter defaulted to (128,128) \n",
    "dataset_train = data1.Dataset(train_data_dir, train_data_filename)\n",
    "\n",
    "dataset_val = data1.Dataset(val_data_dir, val_data_filename)\n",
    "# this method switches a dataset to a raw mode, i.e. after dataset.eval()\n",
    "# dataset[idx] stably produces the idx'th original image\n",
    "dataset_val.eval()\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = 4, num_workers=1, \n",
    "    pin_memory=True, persistent_workers=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_train, batch_size = 4, num_workers=1, \n",
    "    pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85befb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             896\n",
      "              ReLU-2         [-1, 32, 128, 128]               0\n",
      "       BatchNorm2d-3         [-1, 32, 128, 128]              64\n",
      "            Conv2d-4         [-1, 32, 128, 128]           9,248\n",
      "              ReLU-5         [-1, 32, 128, 128]               0\n",
      "       BatchNorm2d-6         [-1, 32, 128, 128]              64\n",
      "         MaxPool2d-7           [-1, 32, 64, 64]               0\n",
      "         DownBlock-8  [[-1, 32, 64, 64], [-1, 32, 128, 128]]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]          18,496\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-11           [-1, 64, 64, 64]             128\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-14           [-1, 64, 64, 64]             128\n",
      "        MaxPool2d-15           [-1, 64, 32, 32]               0\n",
      "        DownBlock-16  [[-1, 64, 32, 32], [-1, 64, 64, 64]]               0\n",
      "           Conv2d-17          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-18          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-19          [-1, 128, 32, 32]             256\n",
      "           Conv2d-20          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-21          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-22          [-1, 128, 32, 32]             256\n",
      "        MaxPool2d-23          [-1, 128, 16, 16]               0\n",
      "        DownBlock-24  [[-1, 128, 16, 16], [-1, 128, 32, 32]]               0\n",
      "           Conv2d-25          [-1, 256, 16, 16]         295,168\n",
      "             ReLU-26          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-27          [-1, 256, 16, 16]             512\n",
      "           Conv2d-28          [-1, 256, 16, 16]         590,080\n",
      "             ReLU-29          [-1, 256, 16, 16]               0\n",
      "      BatchNorm2d-30          [-1, 256, 16, 16]             512\n",
      "        DownBlock-31  [[-1, 256, 16, 16], [-1, 256, 16, 16]]               0\n",
      "  ConvTranspose2d-32          [-1, 128, 32, 32]         131,200\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-34          [-1, 128, 32, 32]             256\n",
      "      Concatenate-35          [-1, 256, 32, 32]               0\n",
      "           Conv2d-36          [-1, 128, 32, 32]         295,040\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-38          [-1, 128, 32, 32]             256\n",
      "           Conv2d-39          [-1, 128, 32, 32]         147,584\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-41          [-1, 128, 32, 32]             256\n",
      "          UpBlock-42          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-43           [-1, 64, 64, 64]          32,832\n",
      "             ReLU-44           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-45           [-1, 64, 64, 64]             128\n",
      "      Concatenate-46          [-1, 128, 64, 64]               0\n",
      "           Conv2d-47           [-1, 64, 64, 64]          73,792\n",
      "             ReLU-48           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-49           [-1, 64, 64, 64]             128\n",
      "           Conv2d-50           [-1, 64, 64, 64]          36,928\n",
      "             ReLU-51           [-1, 64, 64, 64]               0\n",
      "      BatchNorm2d-52           [-1, 64, 64, 64]             128\n",
      "          UpBlock-53           [-1, 64, 64, 64]               0\n",
      "  ConvTranspose2d-54         [-1, 32, 128, 128]           8,224\n",
      "             ReLU-55         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-56         [-1, 32, 128, 128]              64\n",
      "      Concatenate-57         [-1, 64, 128, 128]               0\n",
      "           Conv2d-58         [-1, 32, 128, 128]          18,464\n",
      "             ReLU-59         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-60         [-1, 32, 128, 128]              64\n",
      "           Conv2d-61         [-1, 32, 128, 128]           9,248\n",
      "             ReLU-62         [-1, 32, 128, 128]               0\n",
      "      BatchNorm2d-63         [-1, 32, 128, 128]              64\n",
      "          UpBlock-64         [-1, 32, 128, 128]               0\n",
      "           Conv2d-65          [-1, 3, 128, 128]              99\n",
      "================================================================\n",
      "Total params: 1,928,931\n",
      "Trainable params: 1,928,931\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 131.12\n",
      "Params size (MB): 7.36\n",
      "Estimated Total Size (MB): 138.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels = 3, out_channels=3, n_blocks = 4).to(device)\n",
    "summary(model, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa89742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optim_params = model.parameters()\n",
    "optim = torch.optim.Adam(optim_params, lr=lr)\n",
    "# loss = torch.nn.CrossEntropyLoss()\n",
    "loss = modeltools1.FocalLoss(reduction=\"mean\")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma =0.9999)\n",
    "epochs = 1000 #set to at least 30 000 to reproduce the pretrained weights performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c050f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model is loaded if path to weights is specified, otherwise trains itself\n",
    "\n",
    "losses = []\n",
    "if (\"weights_path\" in locals() or \"weights_path\" in globals()) and weights_path:\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    modeltools1.train_model(model, loss, optim, scheduler, dataloader_train,\n",
    "        num_epochs = epochs, device = device) #30 000 takes one hour and suffices\n",
    "    model.cpu()\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    torch.save(model.state_dict(), './weights'+current_time+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce27c33",
   "metadata": {},
   "source": [
    "Проверим на одном изображении.\n",
    "\n",
    "Я провожу большинство проверок на train'е, так как на момент вечера воскресенья, валидационный датасет все еще был битый (маски не совпадают по размеру с исходным изораженим)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ec68c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0935c26d8d194232b031940e1ce63143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8979603275315912\n"
     ]
    }
   ],
   "source": [
    "dataset_train.eval()\n",
    "image = dataset_train[0][0]\n",
    "label = dataset_train[0][1]\n",
    "label_masks = modeltools1.classes_to_masks(label)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "prediction = modeltools1.tiled_eval(model, image, 16, (128,128), (64,64), device, device)\n",
    "pred_mask = modeltools1.logits_to_masks(prediction)\n",
    "masked = modeltools1.apply_mask(image, pred_mask, colors=modeltools1.DEFAULT_OVERLAY_COLORS, alpha=0.3)\n",
    "torchvision.io.write_png(masked, out_dir+\"/test.png\")\n",
    "score = modeltools1.F1_score(pred_mask, label_masks)\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1d00ce",
   "metadata": {},
   "source": [
    "Ну и посчитаем срденее по трейнсету. При этом в папке `out1` появятся: исходные изображения, сгенерированные и эталонные маски, оверлей обоих масок на исходное изображение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78df8c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3ea2c1d29246dca8ffd80de5e51b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images in dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2435dbca4f464eeaa21c37a7c87b3959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.8979603275315912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90daeca7df22452295ab034284e04460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.9183278604105028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c270c988334f8bbe390777ad856f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 2: 0.9096528753406101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773ede8a2a9448fabe1797f3b8267793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 3: 0.9245349943980967\n",
      "Average F1 score on train set: 0.9126190144202002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_train.eval()\n",
    "total_score = modeltools1.eval_on_dataset(model, dataset_train, out_dir+\"/train\", 8,\n",
    "    (128,128), (64,64), device, device)\n",
    "\n",
    "print(f\"Average F1 score on train set: {total_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ed6a7",
   "metadata": {},
   "source": [
    "`Average F1 score on train set: 0.9126190144202002` выглядит уже неплохо, но вот непосредственное изучение результата показывает проблему:\n",
    "![](prob_demo1.png)\n",
    "\n",
    "Сеть распознает классы слишком локально. Понятно, что бОльшую сеть, тренируемую на бОльших фрагментах можно от этого отучить, на текущую архитектуру тут надежды не очень много, поэтому отфильтруем эвристически. Сначала попробуем на одном изображении. Тут можно поподбирать параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76842e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e480db3ecd4348c7ba96a1a62bf4e61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score: 0.8979603275315912, filtered score: 0.9297780981072274\n"
     ]
    }
   ],
   "source": [
    "dataset_train.eval()\n",
    "image, label = dataset_train[0]\n",
    "prediction = modeltools1.tiled_eval(model, image, 16, (128,128), (64,64), device, device)\n",
    "pred_mask = modeltools1.logits_to_masks(prediction)\n",
    "label_mask = modeltools1.classes_to_masks(label)\n",
    "score = modeltools1.F1_score(pred_mask, label_mask)\n",
    "\n",
    "pred_fitered = filters1.remove_small_holes(pred_mask, 10, 200)\n",
    "filtered_score = modeltools1.F1_score(pred_fitered, label_mask)\n",
    "\n",
    "print(f\"Original score: {score}, filtered score: {filtered_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71a187",
   "metadata": {},
   "source": [
    "Прогресс налицо, поэтому прогоним на всех изображениях. Результат можно наблюдать в папке out. Здесь привожу один пример.\n",
    "![](prob_demo2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6ea1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5603d272674a9c83a5b0753f57fbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images in dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4dc5a197444bba84e0079479a76936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.929835574907831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313c5fdfac194fd8a3e458c06be83179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.930605983005312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca94fbf159146deacbfe93ff17e853e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 2: 0.9362953003445535\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36193d1a96274e1aa6eeac9d4091c4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 3: 0.9479652156513637\n",
      "Average F1 score after filtering on train set: 0.9361755184772651\n"
     ]
    }
   ],
   "source": [
    "# Compute F1 with filtration on train dataset\n",
    "dataset_train.eval()\n",
    "total_score = modeltools1.eval_on_dataset(model, dataset_train, out_dir+\"/train\", 8,\n",
    "    (128,128), (64,64), device, device, [], True, 10 ,300)\n",
    "\n",
    "print(f\"Average F1 score after filtering on train set: {total_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6f22d",
   "metadata": {},
   "source": [
    "Так же прогоним на валидационных данных, когда их починят...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5884bf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2824cf3be1854a13904ac0ca14b4d4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Images in dataset:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879fe739ac48409ebfae5fe64d2045a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0: 0.9006673085498438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5648c4193543b988110885a78402fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 0.9524839651320565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4c984f088b494eb12321a2ed60dba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1979450, 1974875]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8152/1063548287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m total_score = modeltools1.eval_on_dataset(model, dataset_val, out_dir+\"/val\", 8,\n\u001b[0m\u001b[0;32m      2\u001b[0m     (128,128), (64,64), device, device, [], True, 10, 300)\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Average F1 score on val set: {total_score}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ryoka\\YandexDisk\\ML\\onecell test\\oc_test\\modeltools1.py\u001b[0m in \u001b[0;36meval_on_dataset\u001b[1;34m(model, dataset, output_filename, tile_batch_size, tile_size, tile_step, model_device, merger_device, class_weights, apply_filter, filter_rad, filter_area)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mpred_mask_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_mask_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Score {i}:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Ryoka\\YandexDisk\\ML\\onecell test\\oc_test\\modeltools1.py\u001b[0m in \u001b[0;36mF1_score\u001b[1;34m(prediction, label, class_weights)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchannel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_score\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[0mmodified\u001b[0m \u001b[1;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \"\"\"\n\u001b[1;32m-> 1071\u001b[1;33m     return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n\u001b[0m\u001b[0;32m   1072\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \"\"\"\n\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m     _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[0;32m   1196\u001b[0m                                                  \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1462\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1463\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1464\u001b[1;33m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[0;32m   1465\u001b[0m                                     pos_label)\n\u001b[0;32m   1466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1275\u001b[0m                          str(average_options))\n\u001b[0;32m   1276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m     \u001b[1;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1979450, 1974875]"
     ]
    }
   ],
   "source": [
    "total_score = modeltools1.eval_on_dataset(model, dataset_val, out_dir+\"/val\", 8,\n",
    "    (128,128), (64,64), device, device, [], True, 10, 300)\n",
    "\n",
    "print(f\"Average F1 score on val set: {total_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe85f",
   "metadata": {},
   "source": [
    "### Выводы ###\n",
    "\n",
    "Видно, что модель дает хорошее начальное приближение для последующей обработки эвристическими методами. Предложенная фильтрация далека от идеала (на самом деле я попробовал первую пришедшую в голову идею и результат меня устроил). \n",
    "\n",
    "В последней момент мне пришла в голову идея вероятностного фильтра для того, чтобы сделать разметку более \"полигональной\", но реализовать я не успеваю. Вместо closing-фильтра сделать следующее: кидать в карту случайным образом маленькие треугольники. Скажем, со сторонами до 10 (20? 30?) пикселей. Если все три вершины одного цвета (на исходной карте без учета уже накиданных треугольников), то на новой карте весь треугольник красится в тот же цвет. Это должно и \"замкнуть\" малые \"дыры\" и сделать границы хотя бы ломанной. В случае наложения двух таких треугольников \"побеждает\" более приоритетный слой (дома над лесами, леса над дефолтом). А потом може еще линеаризовать границы так: запустить детектор границ (Лаплас?), а потом в найденные точки граници покидать отрезки. Дальше смотрим на k-пиксельную окретсность отрезка (k~3-10) и считаем в ней точки разных классов. Если соотношение примерно 1:1:0.01, значит эта прямая разделяет первые два класса два класса. Заливаем полоину ее окретсности в один цвет, вторую -- в другой и вокруг начала и конца чуть сглаживаем углы (гауссом?). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f339b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
